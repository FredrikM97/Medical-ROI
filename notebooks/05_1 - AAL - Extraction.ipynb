{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from torchcam.cams import GradCAMpp, SmoothGradCAMpp\n",
    "from scipy.ndimage import zoom\n",
    "from nilearn.datasets import fetch_atlas_aal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = True\n",
    "train_full = True\n",
    "train_CV = True\n",
    "test_model = False\n",
    "produce_CAM = False\n",
    "generateSplits = False\n",
    "\n",
    "folder = 'k10b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal_img = nib.load('./AAL/AAL.nii').get_fdata()[5:85, 8:103, 3:80]\n",
    "\n",
    "file = open(\"./AAL/labels.pkl\", \"rb\")\n",
    "aal_labels = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNIsetPreloaded(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, classes, include):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._prepare_mask(include)\n",
    "        \n",
    "        self.classes = classes\n",
    "        self.labels = labels\n",
    "        self.images = self._mask_images(images)\n",
    "        \n",
    "    def getClassCounts(self):\n",
    "        counts = {0: 0, 1: 0, 2: 0}\n",
    "        for label in self.labels:\n",
    "            counts[label] += 1\n",
    "        return counts\n",
    "    \n",
    "    def showImage(self, idx):\n",
    "        plt.imshow(self.images[idx][0, :, :, 45])\n",
    "        \n",
    "    def _prepare_mask(self, include):\n",
    "        self.mask = aal_img == -1\n",
    "        for region in aal_labels.keys():\n",
    "            if not region in include:\n",
    "                self.mask = self.mask | (aal_img == aal_labels[region])\n",
    "        \n",
    "    def _compute_labels(self, imagePaths, classes):\n",
    "        labels = []\n",
    "        for imagePath in imagePaths:\n",
    "            labels.append(classes[imagePath.split('/')[-2]])\n",
    "        return labels\n",
    "    \n",
    "    def _mask_images(self, images):\n",
    "        for image in images:\n",
    "            image[0][self.mask] = 0\n",
    "        return images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllImages(shuffler):\n",
    "    path = '../thesis-data2/ADNI_Soft/'\n",
    "    cn_files = os.listdir(path + 'CN/')\n",
    "    mci_files = os.listdir(path + 'MCI/')\n",
    "    ad_files = os.listdir(path + 'AD/')\n",
    "    \n",
    "    for category, files in zip(['CN', 'MCI', 'AD'], [cn_files, mci_files, ad_files]):\n",
    "        for i in range(len(files)):\n",
    "            files[i] = path + category + '/' + files[i]\n",
    "    \n",
    "    scaler = (torch.linspace(-1, 1, aal_img.shape[0]), torch.linspace(-1, 1, aal_img.shape[1]), torch.linspace(-1, 1, aal_img.shape[2]))\n",
    "    meshz, meshy, meshx = torch.meshgrid(scaler)\n",
    "    grid = torch.stack((meshx, meshy, meshz), 3)\n",
    "    grid = grid.unsqueeze(0)\n",
    "    rescaler = lambda x: F.grid_sample(x, grid, align_corners = True)\n",
    "        \n",
    "    images = []\n",
    "    for file in cn_files + mci_files + ad_files:\n",
    "        x = torch.from_numpy(nib.load(file).get_fdata()[np.newaxis, :, :, :]).float()\n",
    "        x = rescaler(x[np.newaxis, :, :, :, :])[0].numpy()\n",
    "        images.append(x)\n",
    "        \n",
    "    images = np.array(images)\n",
    "    labels = np.array([0] * len(cn_files) + [1] * len(mci_files) + [2] * len(ad_files))\n",
    "    \n",
    "    return images[shuffler], labels[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(images, labels, ratio):\n",
    "    skf = StratifiedKFold(ratio, shuffle = True)\n",
    "    \n",
    "    for train_idxs, test_idxs in skf.split(images, labels):\n",
    "        train_images = images[train_idxs]\n",
    "        train_labels = labels[train_idxs]\n",
    "        all_train_idxs = train_idxs\n",
    "        break\n",
    "        \n",
    "    return all_train_idxs, test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldData(images, labels):\n",
    "    skf = StratifiedKFold(10, shuffle = True)\n",
    "        \n",
    "    train_splits = []\n",
    "    val_splits = []\n",
    "    \n",
    "    for train_idxs, val_idxs in skf.split(images, labels):\n",
    "        train_splits.append(train_idxs)\n",
    "        val_splits.append(val_idxs)\n",
    "        \n",
    "    return train_splits, val_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSet(images, labels, include):\n",
    "    classes = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "\n",
    "    dataset = ADNIsetPreloaded(images, labels, classes, include = include)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size = 6, shuffle = True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv3d(1, 16, kernel_size = (3, 3, 3))\n",
    "        self.pool_1 = nn.MaxPool3d((2, 2, 2))\n",
    "        self.batch_1 = nn.BatchNorm3d(16)\n",
    "        \n",
    "        self.conv_2 = nn.Conv3d(16, 32, kernel_size = (3, 3, 3))\n",
    "        self.pool_2 = nn.MaxPool3d((2, 2, 2))\n",
    "        self.batch_2 = nn.BatchNorm3d(32)\n",
    "        \n",
    "        self.conv_3 = nn.Conv3d(32, 64, kernel_size = (3, 3, 3))\n",
    "        self.pool_3 = nn.MaxPool3d((2, 2, 2))\n",
    "        self.batch_3 = nn.BatchNorm3d(64)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(35840, 128)\n",
    "        self.fc_2 = nn.Linear(128, 64)\n",
    "        self.fc_3 = nn.Linear(64, 3)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.batch_1(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = self.batch_2(x)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.pool_3(x)\n",
    "        x = self.batch_3(x)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        x = self.fc_1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc_3(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareNet(classCounts, cn = 1, mci = 1, ad = 1):\n",
    "    net = Net().cuda()\n",
    "    balance = max(classCounts.values())\n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.tensor([cn * classCounts[0] / balance, mci * classCounts[1] / balance, ad * classCounts[2] / balance]).cuda())\n",
    "\n",
    "    if train_model:\n",
    "        optimizer = torch.optim.Adadelta(net.parameters(), lr = 0.01, weight_decay = 0.00001)\n",
    "    else:\n",
    "        optimizer = None\n",
    "        \n",
    "    return net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(net, criterion, optimizer, trainloader, valloader, epochs = 35, verbose = True, save = True):\n",
    "    if train_model:\n",
    "        \n",
    "        best_val_acc_epoch = 0\n",
    "        best_val_acc = 0\n",
    "        best_val_acc_raw_outputs = None\n",
    "        best_val_acc_labels = None\n",
    "        \n",
    "        best_val_auc_epoch = 0\n",
    "        best_val_auc = 0\n",
    "        best_val_auc_raw_outputs = None\n",
    "        best_val_auc_labels = None\n",
    "        \n",
    "        best_val_loss_epoch = 0\n",
    "        best_val_loss = 1000000\n",
    "        best_val_loss_raw_outputs = None\n",
    "        best_val_loss_labels = None\n",
    "        \n",
    "        startTime = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            \n",
    "            train_raw_outputs = np.zeros((len(trainloader.dataset), 3))\n",
    "            train_outputs = np.zeros((len(trainloader.dataset), 3))\n",
    "            train_predictions = np.zeros((len(trainloader.dataset)))\n",
    "            train_labels = np.zeros((len(trainloader.dataset)))\n",
    "\n",
    "            val_raw_outputs = np.zeros((len(valloader.dataset), 3))\n",
    "            val_outputs = np.zeros((len(valloader.dataset), 3))\n",
    "            val_predictions = np.zeros((len(valloader.dataset)))\n",
    "            val_labels = np.zeros((len(valloader.dataset)))\n",
    "\n",
    "            net.train()\n",
    "            for i, data in enumerate(trainloader):\n",
    "                inputs = data[0].cuda()\n",
    "                labels = data[1].cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(inputs.cuda())\n",
    "\n",
    "                loss = criterion(outputs, labels.cuda())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_correct += torch.sum(torch.argmax(outputs, dim = 1) == labels)\n",
    "                \n",
    "                for j, (output, label) in enumerate(zip(outputs, labels)):\n",
    "                    train_raw_outputs[i * trainloader.batch_size + j] = output.detach().cpu().numpy()\n",
    "                    train_outputs[i * trainloader.batch_size + j] = nn.functional.softmax(output, dim = 0).detach().cpu().numpy()\n",
    "                    train_predictions[i * trainloader.batch_size + j] = torch.argmax(output, dim = 0).cpu().numpy()\n",
    "                    train_labels[i * trainloader.batch_size + j] = label.cpu().numpy()\n",
    "\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(valloader):\n",
    "                    inputs = data[0].cuda()\n",
    "                    labels = data[1].cuda()\n",
    "\n",
    "                    outputs = net(inputs.cuda())\n",
    "                    loss = criterion(outputs, labels.cuda())\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    val_correct += torch.sum(torch.argmax(outputs, dim = 1) == labels)\n",
    "\n",
    "                    for j, (output, label) in enumerate(zip(outputs, labels)):\n",
    "                        val_raw_outputs[i * valloader.batch_size + j] = output.detach().cpu().numpy()\n",
    "                        val_outputs[i * valloader.batch_size + j] = nn.functional.softmax(output, dim = 0).detach().cpu().numpy()\n",
    "                        val_predictions[i * valloader.batch_size + j] = torch.argmax(output, dim = 0).cpu().numpy()\n",
    "                        val_labels[i * valloader.batch_size + j] = label.cpu().numpy()\n",
    "                    \n",
    "            if verbose:\n",
    "                print('T-' + str(int(((time.time() - startTime) / (epoch + 1)) * (epochs - epoch - 1))), 'Epoch:', epoch + 1,\n",
    "                      '~ Train Loss:', int(1000 * train_loss / len(trainloader)) / 1000,\n",
    "                      '~ Train Acc:', int(1000 * train_correct / len(trainloader.dataset)) / 1000,\n",
    "                      '~ Train AuC:', int(1000 * roc_auc_score(train_labels, train_outputs, multi_class = 'ovo')) / 1000,\n",
    "                      '~ Val Loss:', int(1000 * val_loss / len(valloader)) / 1000,\n",
    "                      '~ Val Acc:', int(1000 * val_correct / len(valloader.dataset)) / 1000,\n",
    "                      '~ Val AuC:', int(1000 * roc_auc_score(val_labels, val_outputs, multi_class = 'ovo')) / 1000)\n",
    "\n",
    "            if val_correct / len(valloader.dataset) > best_val_acc:\n",
    "                best_val_acc_epoch = epoch\n",
    "                best_val_acc = val_correct / len(valloader.dataset)\n",
    "                best_val_acc_raw_outputs = val_raw_outputs\n",
    "                best_val_acc_labels = val_labels\n",
    "                \n",
    "                if save:\n",
    "                    torch.save(net.state_dict(), 'model.pt')\n",
    "                if verbose:\n",
    "                    print('Saving new best model')\n",
    "                    \n",
    "            if roc_auc_score(val_labels, val_outputs, multi_class = 'ovo') > best_val_auc:\n",
    "                best_val_auc_epoch = epoch\n",
    "                best_val_auc = roc_auc_score(val_labels, val_outputs, multi_class = 'ovo')\n",
    "                best_val_auc_raw_outputs = val_raw_outputs\n",
    "                best_val_auc_labels = val_labels\n",
    "                    \n",
    "            if val_loss / len(valloader) < best_val_loss:\n",
    "                best_val_loss_epoch = epoch\n",
    "                best_val_loss = val_loss / len(valloader)\n",
    "                best_val_loss_raw_outputs = val_raw_outputs\n",
    "                best_val_loss_labels = val_labels\n",
    "\n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                print(confusion_matrix(np.array(val_labels).flatten(), np.array(val_predictions).flatten()))\n",
    "                \n",
    "        print('Best accuracy (', best_val_acc, ') during epoch', best_val_acc_epoch, '. Best AuC (', best_val_auc, ') during epoch', best_val_auc_epoch, '. Best Loss (', best_val_loss, ') during epoch', best_val_loss_epoch)\n",
    "        return best_val_acc_raw_outputs, best_val_acc_labels, best_val_auc_raw_outputs, best_val_auc_labels, best_val_loss_raw_outputs, best_val_loss_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.load(folder + '/shuffler.npy') \n",
    "\n",
    "images, labels = getAllImages(shuffler)\n",
    "\n",
    "if generateSplits:\n",
    "    all_train_idxs, test_idxs = splitData(images, labels, 10)\n",
    "    train_splits, val_splits = foldData(images[all_train_idxs], labels[all_train_idxs])\n",
    "    pure_train_idxs, pure_val_idxs = splitData(images[all_train_idxs], labels[all_train_idxs], 5)\n",
    "    \n",
    "    with open('train_splits.pickle', 'wb') as fp:\n",
    "        pickle.dump(train_splits, fp)\n",
    "    with open('val_splits.pickle', 'wb') as fp:\n",
    "        pickle.dump(val_splits, fp)\n",
    "    with open('all_train_idxs.pickle', 'wb') as fp:\n",
    "        pickle.dump(all_train_idxs, fp)\n",
    "    with open('test_idxs.pickle', 'wb') as fp:\n",
    "        pickle.dump(test_idxs, fp)\n",
    "    with open('pure_train_idxs.pickle', 'wb') as fp:\n",
    "        pickle.dump(pure_train_idxs, fp)\n",
    "    with open('pure_val_idxs.pickle', 'wb') as fp:\n",
    "        pickle.dump(pure_val_idxs, fp)\n",
    "        \n",
    "else:\n",
    "    with open(folder + '/train_splits.pickle', 'rb') as fp:\n",
    "        train_splits = pickle.load(fp)\n",
    "    with open(folder + '/val_splits.pickle', 'rb') as fp:\n",
    "        val_splits = pickle.load(fp)\n",
    "    with open(folder + '/all_train_idxs.pickle', 'rb') as fp:\n",
    "        all_train_idxs = pickle.load(fp)\n",
    "    with open(folder + '/test_idxs.pickle', 'rb') as fp:\n",
    "        test_idxs = pickle.load(fp)\n",
    "    with open(folder + '/pure_train_idxs.pickle', 'rb') as fp:\n",
    "        pure_train_idxs = pickle.load(fp)\n",
    "    with open(folder + '/pure_val_idxs.pickle', 'rb') as fp:\n",
    "        pure_val_idxs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_all = list(aal_labels.keys())\n",
    "exclude_background = list(aal_labels.keys())\n",
    "exclude_background.remove('Background')\n",
    "\n",
    "include_left = ['Precentral_L', 'Frontal_Sup_L', 'Frontal_Sup_Orb_L',\n",
    "       'Frontal_Mid_L', 'Frontal_Mid_Orb_L', 'Frontal_Inf_Oper_L',\n",
    "       'Frontal_Inf_Tri_L', 'Frontal_Inf_Orb_L', 'Rolandic_Oper_L',\n",
    "       'Supp_Motor_Area_L', 'Olfactory_L', 'Frontal_Sup_Medial_L',\n",
    "       'Frontal_Med_Orb_L', 'Rectus_L', 'Insula_L', 'Cingulum_Ant_L',\n",
    "       'Cingulum_Mid_L', 'Cingulum_Post_L', 'Hippocampus_L',\n",
    "       'ParaHippocampal_L', 'Amygdala_L', 'Calcarine_L', 'Cuneus_L',\n",
    "       'Lingual_L', 'Occipital_Sup_L', 'Occipital_Mid_L',\n",
    "       'Occipital_Inf_L', 'Fusiform_L', 'Postcentral_L', 'Parietal_Sup_L',\n",
    "       'Parietal_Inf_L', 'SupraMarginal_L', 'Angular_L', 'Precuneus_L',\n",
    "       'Paracentral_Lobule_L', 'Caudate_L', 'Putamen_L', 'Pallidum_L',\n",
    "       'Thalamus_L', 'Heschl_L', 'Temporal_Sup_L', 'Temporal_Pole_Sup_L',\n",
    "       'Temporal_Mid_L', 'Temporal_Pole_Mid_L', 'Temporal_Inf_L',\n",
    "       'Cerebelum_Crus1_L', 'Cerebelum_Crus2_L', 'Cerebelum_3_L',\n",
    "       'Cerebelum_4_5_L', 'Cerebelum_6_L', 'Cerebelum_7b_L',\n",
    "       'Cerebelum_8_L', 'Cerebelum_9_L', 'Cerebelum_10_L']\n",
    "\n",
    "include_right = ['Precentral_R', 'Frontal_Sup_R', 'Frontal_Sup_Orb_R',\n",
    "       'Frontal_Mid_R', 'Frontal_Mid_Orb_R', 'Frontal_Inf_Oper_R',\n",
    "       'Frontal_Inf_Tri_R', 'Frontal_Inf_Orb_R', 'Rolandic_Oper_R',\n",
    "       'Supp_Motor_Area_R', 'Olfactory_R', 'Frontal_Sup_Medial_R',\n",
    "       'Frontal_Med_Orb_R', 'Rectus_R', 'Insula_R', 'Cingulum_Ant_R',\n",
    "       'Cingulum_Mid_R', 'Cingulum_Post_R', 'Hippocampus_R',\n",
    "       'ParaHippocampal_R', 'Amygdala_R', 'Calcarine_R', 'Cuneus_R',\n",
    "       'Lingual_R', 'Occipital_Sup_R', 'Occipital_Mid_R',\n",
    "       'Occipital_Inf_R', 'Fusiform_R', 'Postcentral_R', 'Parietal_Sup_R',\n",
    "       'Parietal_Inf_R', 'SupraMarginal_R', 'Angular_R', 'Precuneus_R',\n",
    "       'Paracentral_Lobule_R', 'Caudate_R', 'Putamen_R', 'Pallidum_R',\n",
    "       'Thalamus_R', 'Heschl_R', 'Temporal_Sup_R', 'Temporal_Pole_Sup_R',\n",
    "       'Temporal_Mid_R', 'Temporal_Pole_Mid_R', 'Temporal_Inf_R',\n",
    "       'Cerebelum_Crus1_R', 'Cerebelum_Crus2_R', 'Cerebelum_3_R',\n",
    "       'Cerebelum_4_5_R', 'Cerebelum_6_R', 'Cerebelum_7b_R',\n",
    "       'Cerebelum_8_R', 'Cerebelum_9_R', 'Cerebelum_10_R']\n",
    "\n",
    "if train_model:\n",
    "    \n",
    "    if train_full:\n",
    "    \n",
    "        if train_CV:\n",
    "            results = {}\n",
    "            \n",
    "            for name, inclusion in zip(['left', 'right'], [include_left, include_right]):\n",
    "\n",
    "                for i, (train_idxs, val_idxs) in enumerate(zip(train_splits, val_splits)):\n",
    "                    print('Split', i)\n",
    "\n",
    "                    trainloader = createSet(images[all_train_idxs][train_idxs], labels[all_train_idxs][train_idxs], inclusion)\n",
    "                    valloader = createSet(images[all_train_idxs][val_idxs], labels[all_train_idxs][val_idxs], inclusion)\n",
    "\n",
    "                    net, criterion, optimizer = prepareNet(trainloader.dataset.getClassCounts())\n",
    "                    acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels = trainNet(net, criterion, optimizer, trainloader, valloader, epochs = 50, verbose = False, save = False)\n",
    "\n",
    "                    results[i] = (acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels)\n",
    "                    with open(name + '_results.pickle', 'wb') as fp:\n",
    "                        pickle.dump(results, fp)\n",
    "                    \n",
    "        else:\n",
    "            trainloader = createSet(images[all_train_idxs][pure_train_idxs], labels[all_train_idxs][pure_train_idxs], include_all)\n",
    "            valloader = createSet(images[all_train_idxs][pure_val_idxs], labels[all_train_idxs][pure_val_idxs], include_all)\n",
    "            net, criterion, optimizer = prepareNet(trainloader.dataset.getClassCounts())\n",
    "            trainNet(net, criterion, optimizer, trainloader, valloader, epochs = 50, verbose = True, save = True)\n",
    "            \n",
    "    else:\n",
    "        results = {}\n",
    "        \n",
    "        with open(folder + '/stats.npy', 'rb') as fp:\n",
    "            rankings = pickle.load(fp)\n",
    "        \n",
    "        rankedRegions = list(rankings['All']['Intensities'].keys())\n",
    "        rankedRegions.remove('Background')\n",
    "        \n",
    "        for i in range(90, 9, -10):\n",
    "            print('Ignoring the worst', i, 'regions')\n",
    "            \n",
    "            for j, (train_idxs, val_idxs) in enumerate(zip(train_splits, val_splits)):\n",
    "                print('Split', j)\n",
    "\n",
    "                trainloader = createSet(images[all_train_idxs][train_idxs], labels[all_train_idxs][train_idxs], rankedRegions[-(116-i):])\n",
    "                valloader = createSet(images[all_train_idxs][val_idxs], labels[all_train_idxs][val_idxs], rankedRegions[-(116-i):])\n",
    "\n",
    "                net, criterion, optimizer = prepareNet(trainloader.dataset.getClassCounts())\n",
    "                acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels = trainNet(net, criterion, optimizer, trainloader, valloader, epochs = 50, verbose = False, save = False)\n",
    "                \n",
    "                results[(i, j)] = (acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels)\n",
    "                with open('sub_results_normal.pickle', 'wb') as fp:\n",
    "                    pickle.dump(results, fp)\n",
    "                    \n",
    "        results = {}\n",
    "        \n",
    "        rankedRegions.reverse()\n",
    "        \n",
    "        for i in range(90, 9, -10):\n",
    "            print('Ignoring the best', i, 'regions')\n",
    "            \n",
    "            for j, (train_idxs, val_idxs) in enumerate(zip(train_splits, val_splits)):\n",
    "                print('Split', j)\n",
    "\n",
    "                trainloader = createSet(images[all_train_idxs][train_idxs], labels[all_train_idxs][train_idxs], rankedRegions[-(116-i):])\n",
    "                valloader = createSet(images[all_train_idxs][val_idxs], labels[all_train_idxs][val_idxs], rankedRegions[-(116-i):])\n",
    "\n",
    "                net, criterion, optimizer = prepareNet(trainloader.dataset.getClassCounts())\n",
    "                acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels = trainNet(net, criterion, optimizer, trainloader, valloader, epochs = 50, verbose = False, save = False)\n",
    "                \n",
    "                results[(i, j)] = (acc_outputs, acc_labels, auc_outputs, auc_labels, loss_outputs, loss_labels)\n",
    "                with open('sub_results_reverse.pickle', 'wb') as fp:\n",
    "                    pickle.dump(results, fp)\n",
    "        \n",
    "else:\n",
    "    net = Net().cuda()\n",
    "    net.load_state_dict(torch.load(folder + '/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_model:\n",
    "    \n",
    "    #testloader = createSet(images[test_idxs], labels[test_idxs], include_all)\n",
    "    #testloader = createSet(images[all_train_idxs][pure_train_idxs], labels[all_train_idxs][pure_train_idxs], include_all)\n",
    "    #net.load_state_dict(torch.load('model.pt'))\n",
    "    net, criterion, optimizer = prepareNet(testloader.dataset.getClassCounts())\n",
    "    net.load_state_dict(torch.load(folder + '/ensamble/model_' + 'ia_1' + '.pt'))\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    test_outputs = np.zeros((len(testloader.dataset), 3))\n",
    "    test_predictions = np.zeros((len(testloader.dataset)))\n",
    "    test_labels = np.zeros((len(testloader.dataset)))\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs = data[0].cuda()\n",
    "            labels = data[1].cuda()\n",
    "\n",
    "            outputs = net(inputs.cuda())\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += torch.sum(torch.argmax(outputs, dim = 1) == labels)\n",
    "            \n",
    "            for j, (output, label) in enumerate(zip(outputs, labels)):\n",
    "                test_outputs[i * testloader.batch_size + j] = nn.functional.softmax(output, dim = 0).detach().cpu().numpy()\n",
    "                test_predictions[i * testloader.batch_size + j] = torch.argmax(output, dim = 0).cpu().numpy()\n",
    "                test_labels[i * testloader.batch_size + j] = label.cpu().numpy()\n",
    "\n",
    "    print('Test Loss:', int(1000 * test_loss / len(testloader)) / 1000,\n",
    "          '~ Test Acc:', int(1000 * test_correct / len(testloader.dataset)) / 1000,\n",
    "          '~ Val AuC:', int(1000 * roc_auc_score(test_labels, test_outputs, multi_class = 'ovo')) / 1000)\n",
    "\n",
    "    print(confusion_matrix(np.array(test_labels).flatten(), np.array(test_predictions).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = createSet(images[all_train_idxs][pure_val_idxs], labels[all_train_idxs][pure_val_idxs], include_all).dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamble = 'ia_10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if produce_CAM:\n",
    "    \n",
    "    net = Net().cuda()\n",
    "    net.load_state_dict(torch.load(folder + '/ensamble/model_' + ensamble + '.pt'))\n",
    "    net.eval()\n",
    "    \n",
    "    cam_extractor = GradCAMpp(net, input_shape = [1, aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]], target_layer = 'conv_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if produce_CAM:\n",
    "\n",
    "    combined_activation_map = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_CN = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_MCI = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_AD = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    \n",
    "    combined_activation_map_wrong = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_wrong_CN = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_wrong_MCI = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    combined_activation_map_wrong_AD = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    \n",
    "    overlap_activation_map = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_CN = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_MCI = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_AD = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    \n",
    "    overlap_activation_map_wrong = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_wrong_CN = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_wrong_MCI = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    overlap_activation_map_wrong_AD = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "    \n",
    "    image_count_All = 0\n",
    "    image_count_CN = 0\n",
    "    image_count_MCI = 0\n",
    "    image_count_AD = 0\n",
    "    \n",
    "    image_count_wrong_All = 0\n",
    "    image_count_wrong_CN = 0\n",
    "    image_count_wrong_MCI = 0\n",
    "    image_count_wrong_AD = 0\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images.images, images.labels)):\n",
    "        \n",
    "        x = torch.from_numpy(image).cuda()[np.newaxis, :, :, :, :]\n",
    "\n",
    "        class_scores = net(x)\n",
    "        class_idx = class_scores.squeeze(0).argmax().item()\n",
    "        \n",
    "        if class_idx == label:\n",
    "            image_count_All += 1\n",
    "            combined_activation_map_all = combined_activation_map\n",
    "            overlap_activation_map_all = overlap_activation_map\n",
    "            if label == 0:\n",
    "                combined_activation_map_condition = combined_activation_map_CN\n",
    "                overlap_activation_map_condition = overlap_activation_map_CN\n",
    "                image_count_CN += 1\n",
    "            elif label == 1:\n",
    "                combined_activation_map_condition = combined_activation_map_MCI\n",
    "                overlap_activation_map_condition = overlap_activation_map_MCI\n",
    "                image_count_MCI += 1\n",
    "            else:\n",
    "                combined_activation_map_condition = combined_activation_map_AD\n",
    "                overlap_activation_map_condition = overlap_activation_map_AD\n",
    "                image_count_AD += 1\n",
    "        else:\n",
    "            image_count_wrong_All += 1\n",
    "            combined_activation_map_all = combined_activation_map_wrong\n",
    "            overlap_activation_map_all = overlap_activation_map_wrong\n",
    "            if label == 0:\n",
    "                combined_activation_map_condition = combined_activation_map_wrong_CN\n",
    "                overlap_activation_map_condition = overlap_activation_map_wrong_CN\n",
    "                image_count_wrong_CN += 1\n",
    "            elif label == 1:\n",
    "                combined_activation_map_condition = combined_activation_map_wrong_MCI\n",
    "                overlap_activation_map_condition = overlap_activation_map_wrong_MCI\n",
    "                image_count_wrong_MCI += 1\n",
    "            else:\n",
    "                combined_activation_map_condition = combined_activation_map_wrong_AD\n",
    "                overlap_activation_map_condition = overlap_activation_map_wrong_AD\n",
    "                image_count_wrong_AD += 1\n",
    "\n",
    "        activation_map = cam_extractor(class_idx, class_scores).cpu().numpy()\n",
    "        scaled_activation_map = zoom(activation_map, (x.shape[2] / activation_map.shape[0], x.shape[3] / activation_map.shape[1], x.shape[4] / activation_map.shape[2]))\n",
    "\n",
    "        zero_mask = x == 0\n",
    "        scaled_activation_map[zero_mask.cpu().numpy()[0, 0, :, :, :]] = 0\n",
    "\n",
    "        normalized_actvation_map = scaled_activation_map# / scaled_activation_map.sum()\n",
    "        \n",
    "        combined_activation_map_all += normalized_actvation_map\n",
    "        combined_activation_map_condition += normalized_actvation_map\n",
    "        \n",
    "        scaled_activation_map[scaled_activation_map > 0] = 1\n",
    "        scaled_activation_map[scaled_activation_map < 0] = 0\n",
    "        \n",
    "        overlap_activation_map_all += scaled_activation_map\n",
    "        overlap_activation_map_condition += scaled_activation_map\n",
    "    \n",
    "    if image_count_All > 0:\n",
    "        combined_activation_map = combined_activation_map / image_count_All\n",
    "    if image_count_CN > 0:\n",
    "        combined_activation_map_CN = combined_activation_map_CN / image_count_CN\n",
    "    if image_count_MCI > 0:\n",
    "        combined_activation_map_MCI = combined_activation_map_MCI / image_count_MCI\n",
    "    if image_count_AD > 0:\n",
    "        combined_activation_map_AD = combined_activation_map_AD / image_count_AD\n",
    "    \n",
    "    if image_count_wrong_All > 0:\n",
    "        combined_activation_map_wrong = combined_activation_map_wrong / image_count_wrong_All\n",
    "    if image_count_wrong_CN > 0:\n",
    "        combined_activation_map_wrong_CN = combined_activation_map_wrong_CN / image_count_wrong_CN\n",
    "    if image_count_wrong_MCI > 0:\n",
    "        combined_activation_map_wrong_MCI = combined_activation_map_wrong_MCI / image_count_wrong_MCI\n",
    "    if image_count_wrong_AD > 0:\n",
    "        combined_activation_map_wrong_AD = combined_activation_map_wrong_AD / image_count_wrong_AD\n",
    "    \n",
    "    if image_count_All > 0:\n",
    "        overlap_activation_map = overlap_activation_map / image_count_All\n",
    "    if image_count_CN > 0:\n",
    "        overlap_activation_map_CN = overlap_activation_map_CN / image_count_CN\n",
    "    if image_count_MCI > 0:\n",
    "        overlap_activation_map_MCI = overlap_activation_map_MCI / image_count_MCI\n",
    "    if image_count_AD > 0:\n",
    "        overlap_activation_map_AD = overlap_activation_map_AD / image_count_AD\n",
    "        \n",
    "    if image_count_wrong_All > 0:\n",
    "        overlap_activation_map_wrong = overlap_activation_map_wrong / image_count_wrong_All\n",
    "    if image_count_wrong_CN > 0:\n",
    "        overlap_activation_map_wrong_CN = overlap_activation_map_wrong_CN / image_count_wrong_CN\n",
    "    if image_count_wrong_MCI > 0:\n",
    "        overlap_activation_map_wrong_MCI = overlap_activation_map_wrong_MCI / image_count_wrong_MCI\n",
    "    if image_count_wrong_AD > 0:\n",
    "        overlap_activation_map_wrong_AD = overlap_activation_map_wrong_AD / image_count_wrong_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if produce_CAM:\n",
    "    np.save(folder + '/ensamble/Map_val_All_' + ensamble + '.npy', combined_activation_map)\n",
    "    np.save(folder + '/ensamble/Map_val_CN_' + ensamble + '.npy', combined_activation_map_CN)\n",
    "    np.save(folder + '/ensamble/Map_val_MCI_' + ensamble + '.npy', combined_activation_map_MCI)\n",
    "    np.save(folder + '/ensamble/Map_val_AD_' + ensamble + '.npy', combined_activation_map_AD)\n",
    "    \n",
    "    np.save(folder + '/ensamble/Map_val_wrong_All_' + ensamble + '.npy', combined_activation_map_wrong)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_CN_' + ensamble + '.npy', combined_activation_map_wrong_CN)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_MCI_' + ensamble + '.npy', combined_activation_map_wrong_MCI)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_AD_' + ensamble + '.npy', combined_activation_map_wrong_AD)\n",
    "    \n",
    "    np.save(folder + '/ensamble/Map_val_All_overlap_' + ensamble + '.npy', overlap_activation_map)\n",
    "    np.save(folder + '/ensamble/Map_val_CN_overlap_' + ensamble + '.npy', overlap_activation_map_CN)\n",
    "    np.save(folder + '/ensamble/Map_val_MCI_overlap_' + ensamble + '.npy', overlap_activation_map_MCI)\n",
    "    np.save(folder + '/ensamble/Map_val_AD_overlap_' + ensamble + '.npy', overlap_activation_map_AD)\n",
    "    \n",
    "    np.save(folder + '/ensamble/Map_val_wrong_All_overlap_' + ensamble + '.npy', overlap_activation_map_wrong)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_CN_overlap_' + ensamble + '.npy', overlap_activation_map_wrong_CN)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_MCI_overlap_' + ensamble + '.npy', overlap_activation_map_wrong_MCI)\n",
    "    np.save(folder + '/ensamble/Map_val_wrong_AD_overlap_' + ensamble + '.npy', overlap_activation_map_wrong_AD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not produce_CAM:\n",
    "    combined_activation_map = np.load(folder + '/Map_All.npy')\n",
    "    combined_activation_map_CN = np.load(folder + '/Map_CN.npy')\n",
    "    combined_activation_map_MCI = np.load(folder + '/Map_MCI.npy')\n",
    "    combined_activation_map_AD = np.load(folder + '/Map_AD.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "cn_average = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "mci_average = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "ad_average = np.zeros((aal_img.shape[0], aal_img.shape[1], aal_img.shape[2]))\n",
    "\n",
    "cn_count = 0\n",
    "mci_count = 0\n",
    "ad_count = 0\n",
    "\n",
    "for image in images:\n",
    "    average += image[0][0]\n",
    "    if image[1] == 0:\n",
    "        cn_average += image[0][0]\n",
    "        cn_count += 1\n",
    "    elif image[1] == 1:\n",
    "        mci_average += image[0][0]\n",
    "        mci_count += 1\n",
    "    else:\n",
    "        ad_average += image[0][0]\n",
    "        ad_count += 1\n",
    "        \n",
    "average = average / (cn_count + mci_count + ad_count)\n",
    "cn_average = cn_average / cn_count\n",
    "mci_average = mci_average / mci_count\n",
    "ad_average = ad_average / ad_count\n",
    "\n",
    "np.save('average.npy', average)\n",
    "np.save('average_CN.npy', cn_average)\n",
    "np.save('average_MCI.npy', mci_average)\n",
    "np.save('average_AD.npy', ad_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_index = 45\n",
    "#vmax = None\n",
    "vmax = max(combined_activation_map_CN.max(), combined_activation_map_MCI.max(), combined_activation_map_AD.max())\n",
    "fig, axs = plt.subplots(3, 6, figsize = (25, 12))\n",
    "axs[0, 0].imshow(combined_activation_map[:, :, slice_index], vmax = vmax)\n",
    "axs[0, 0].set_title('All')\n",
    "axs[0, 1].imshow(combined_activation_map_CN[:, :, slice_index], vmax = vmax)\n",
    "axs[0, 1].set_title('CN')\n",
    "axs[0, 2].imshow(combined_activation_map_MCI[:, :, slice_index], vmax = vmax)\n",
    "axs[0, 2].set_title('MCI')\n",
    "axs[0, 3].imshow(combined_activation_map_AD[:, :, slice_index], vmax = vmax)\n",
    "axs[0, 3].set_title('AD')\n",
    "axs[0, 4].imshow(np.absolute(combined_activation_map_AD - combined_activation_map_CN)[:, :, slice_index], vmax = vmax)\n",
    "axs[0, 4].set_title('|CN - AD|')\n",
    "axs[0, 5].set_visible(False)\n",
    "axs[1, 0].imshow(aal_img[:, :, slice_index], cmap = 'gray')\n",
    "axs[1, 0].imshow(combined_activation_map[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[1, 0].set_title('All')\n",
    "axs[1, 1].imshow(aal_img[:, :, slice_index], cmap = 'gray')\n",
    "axs[1, 1].imshow(combined_activation_map_CN[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[1, 1].set_title('CN')\n",
    "axs[1, 2].imshow(aal_img[:, :, slice_index], cmap = 'gray')\n",
    "axs[1, 2].imshow(combined_activation_map_MCI[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[1, 2].set_title('MCI')\n",
    "axs[1, 3].imshow(aal_img[:, :, slice_index], cmap = 'gray')\n",
    "axs[1, 3].imshow(combined_activation_map_AD[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[1, 3].set_title('AD')\n",
    "axs[1, 4].imshow(aal_img[:, :, slice_index], cmap = 'gray')\n",
    "axs[1, 4].imshow(np.absolute(combined_activation_map_AD - combined_activation_map_CN)[:, :, slice_index], alpha = 0.75, vmax = vmax)\n",
    "axs[1, 4].set_title('|CN - AD|')\n",
    "axs[1, 5].imshow(aal_img[:, :, slice_index])\n",
    "axs[1, 5].set_title('AAL atlas')\n",
    "axs[2, 0].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 0].imshow(combined_activation_map[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[2, 0].set_title('All')\n",
    "axs[2, 1].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 1].imshow(combined_activation_map_CN[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[2, 1].set_title('CN')\n",
    "axs[2, 2].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 2].imshow(combined_activation_map_MCI[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[2, 2].set_title('MCI')\n",
    "axs[2, 3].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 3].imshow(combined_activation_map_AD[:, :, slice_index], alpha = 0.85, vmax = vmax)\n",
    "axs[2, 3].set_title('AD')\n",
    "axs[2, 4].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 4].imshow(np.absolute(combined_activation_map_AD - combined_activation_map_CN)[:, :, slice_index], alpha = 0.75, vmax = vmax)\n",
    "axs[2, 4].set_title('|CN - AD|')\n",
    "axs[2, 5].imshow(average[:, :, slice_index], cmap = 'gray')\n",
    "axs[2, 5].set_title('Average Scan')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if produce_CAM:\n",
    "    \n",
    "    all_stats = {}\n",
    "    \n",
    "    for stats, CAM in zip(['All', 'CN', 'MCI', 'AD', 'AD-CN'], [combined_activation_map, combined_activation_map_CN, combined_activation_map_MCI, combined_activation_map_AD, combined_activation_map_AD - combined_activation_map_CN]):\n",
    "    \n",
    "        volumes = {}\n",
    "        intensities = {}\n",
    "        densities = {}\n",
    "\n",
    "        for key in aal_labels.keys():\n",
    "            mask = aal_img != aal_labels[key]\n",
    "            masked_cam = copy.copy(CAM)\n",
    "            masked_cam[mask] = 0\n",
    "\n",
    "            volumes[key] = mask.size - np.count_nonzero(mask)\n",
    "            intensities[key] = masked_cam.sum()\n",
    "            densities[key] = intensities[key] / volumes[key]\n",
    "\n",
    "        all_stats[stats] = {}\n",
    "        all_stats[stats]['Volume'] = dict(sorted(volumes.items(), key = lambda item: item[1], reverse = False))\n",
    "        all_stats[stats]['Intensities'] = dict(sorted(intensities.items(), key = lambda item: item[1], reverse = False))\n",
    "        all_stats[stats]['Densities'] = dict(sorted(densities.items(), key = lambda item: item[1], reverse = False))\n",
    "\n",
    "    for stats, CAM in zip(['All', 'CN', 'MCI', 'AD', 'AD-CN'], [overlap_activation_map, overlap_activation_map_CN, overlap_activation_map_MCI, overlap_activation_map_AD, overlap_activation_map_AD - overlap_activation_map_CN]):\n",
    "        \n",
    "        overlap = {}\n",
    "        \n",
    "        for key in aal_labels.keys():\n",
    "            mask = aal_img != aal_labels[key]\n",
    "            masked_cam = copy.copy(CAM)\n",
    "            masked_cam[mask] = 0\n",
    "            \n",
    "            overlap[key] = masked_cam.sum() / (mask.size - np.count_nonzero(mask))\n",
    "            \n",
    "        all_stats[stats]['Overlap'] = dict(sorted(overlap.items(), key = lambda item: item[1], reverse = False))\n",
    "\n",
    "    with open('stats.npy', 'wb') as fp:\n",
    "        pickle.dump(all_stats, fp)\n",
    "        \n",
    "else:\n",
    "    with open(folder + '/stats.npy', 'rb') as fp:\n",
    "        all_stats = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = pd.DataFrame(columns = ['Region', 'All Intensity', 'All Intensity Rank', 'CN Intensity', 'CN Intensity Rank', 'MCI Intensity', 'MCI Intensity Rank', 'AD Intensity', 'AD Intensity Rank', 'AD-CN Intensity', 'AD-CN Intensity Rank',\n",
    "                                      'All Overlap', 'All Overlap Rank', 'CN Overlap', 'CN Overlap Rank', 'MCI Overlap', 'MCI Overlap Rank', 'AD Overlap', 'AD Overlap Rank', 'AD-CN Overlap', 'AD-CN Overlap Rank'])\n",
    "\n",
    "all_keys = list(all_stats['All']['Intensities'].keys())\n",
    "cn_keys = list(all_stats['CN']['Intensities'].keys())\n",
    "mci_keys = list(all_stats['MCI']['Intensities'].keys())\n",
    "ad_keys = list(all_stats['AD']['Intensities'].keys())\n",
    "cn_ad_keys = list(all_stats['AD-CN']['Intensities'].keys())\n",
    "\n",
    "overlap_all_keys = list(all_stats['All']['Overlap'].keys())\n",
    "overlap_cn_keys = list(all_stats['CN']['Overlap'].keys())\n",
    "overlap_mci_keys = list(all_stats['MCI']['Overlap'].keys())\n",
    "overlap_ad_keys = list(all_stats['AD']['Overlap'].keys())\n",
    "overlap_cn_ad_keys = list(all_stats['AD-CN']['Overlap'].keys())\n",
    "\n",
    "for key in aal_labels.keys():\n",
    "    all_stats_df = all_stats_df.append({\n",
    "        'Region': key,\n",
    "        'All Intensity': all_stats['All']['Intensities'][key],\n",
    "        'All Intensity Rank': 117 - all_keys.index(key),\n",
    "        'CN Intensity': all_stats['CN']['Intensities'][key],\n",
    "        'CN Intensity Rank': 117 - cn_keys.index(key),\n",
    "        'MCI Intensity': all_stats['MCI']['Intensities'][key],\n",
    "        'MCI Intensity Rank': 117 - mci_keys.index(key),\n",
    "        'AD Intensity': all_stats['AD']['Intensities'][key],\n",
    "        'AD Intensity Rank': 117 - ad_keys.index(key),\n",
    "        'AD-CN Intensity': all_stats['AD-CN']['Intensities'][key],\n",
    "        'AD-CN Intensity Rank': 117 - cn_ad_keys.index(key),\n",
    "        'All Overlap': all_stats['All']['Overlap'][key],\n",
    "        'All Overlap Rank': 117 - overlap_all_keys.index(key),\n",
    "        'CN Overlap': all_stats['CN']['Overlap'][key],\n",
    "        'CN Overlap Rank': 117 - overlap_cn_keys.index(key),\n",
    "        'MCI Overlap': all_stats['MCI']['Overlap'][key],\n",
    "        'MCI Overlap Rank': 117 - overlap_mci_keys.index(key),\n",
    "        'AD Overlap': all_stats['AD']['Overlap'][key],\n",
    "        'AD Overlap Rank': 117 - overlap_ad_keys.index(key),\n",
    "        'AD-CN Overlap': all_stats['AD-CN']['Overlap'][key],\n",
    "        'AD-CN Overlap Rank': 117 - overlap_cn_ad_keys.index(key)\n",
    "    }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df_regions = all_stats_df[all_stats_df['Region'] != 'Background']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'AD-CN'\n",
    "fig, ax = plt.subplots(figsize = (30, 10))\n",
    "ax.bar(np.arange(len(all_stats_df_regions.index)), list(all_stats_df_regions.sort_values(condition + ' Intensity Rank')[condition + ' Intensity']))\n",
    "ax.set_xticks(np.arange(len(all_stats_df_regions.index)))\n",
    "ax.set_xticklabels(list(all_stats_df_regions.sort_values(condition + ' Intensity Rank')['Region']), rotation = 60, ha = 'right')\n",
    "ax.set_yticks([])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'AD-CN'\n",
    "fig, ax = plt.subplots(figsize = (30, 10))\n",
    "ax.bar(np.arange(len(all_stats_df_regions.index)), list(all_stats_df_regions.sort_values(condition + ' Overlap Rank')[condition + ' Overlap']))\n",
    "ax.set_xticks(np.arange(len(all_stats_df_regions.index)))\n",
    "ax.set_xticklabels(list(all_stats_df_regions.sort_values(condition + ' Overlap Rank')['Region']), rotation = 60, ha = 'right')\n",
    "ax.set_yticks([])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'All'\n",
    "fig, ax = plt.subplots(figsize = (30, 10))\n",
    "ax.bar(np.arange(len(all_stats_df_regions.index)), list(all_stats_df_regions.sort_values(condition + ' Intensity Rank')[condition + ' Intensity']))\n",
    "ax.set_xticks(np.arange(len(all_stats_df_regions.index)))\n",
    "ax.set_xticklabels(list(all_stats_df_regions.sort_values(condition + ' Intensity Rank')['Region']), rotation = 60, ha = 'right')\n",
    "ax.set_yticks([])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
